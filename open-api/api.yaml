swagger: "2.0"
info:
  version: 2.9.0
  title: Koverse REST API
  description: >
        Specification for interacting with the Koverse Rest API
host: demo.koverse.com

consumes:
  - application/json
produces:
  - application/json

tags:
  - name: Authentication
    description: |
          Methods for logging in individual users.

          These methods only apply when using Koverse's built-in user management.
  - name: Data Sets
    description: |
          Create, configure, and delete Data Sets.
  #- name: Data Set Schemas
  #  description: |
  #        Data Set Schemas
  - name: Data Set Attributes
    description: |
          List, inspect, and configure masking for data set attributes
  - name: Data Set Permissions
    description: |
          Control access to data in Data Sets.

          Access is controlled on a Group basis and includes actions such as writing to a Data Set, querying, deleting, changing configuration, etc.

          Note that these permissions are distinct from system permission which control things like which Groups can do things like create Data Sets, setup Transforms etc.
  - name: System Permissions
    description: |
          Control access to system functionality.
  - name: Indexing Policies
    description: |
          Control how Koverse indexes data in a data set.

          This includes specifying which attributes to index, how data should be tokenized, and which combinations of attributes should be combined into Composite Indexes.
          The set of available indexes affects which kinds of interactive queries Koverse can support.
  - name: Query
    description: |
          Interactively query Data Sets.

          Queries are designed to retrieve a specific subset of records, in many cases in less than a second.
          Applications can call query methods in response to user requests.
  - name: Records
    description: |
          Write and retrieve individual records and sets of sample records.
  - name: Source Types
    description: |
          List available Source types.
          Source types specify which parameters are available for each type of Source.
  - name: Sources
    description: |
          Sources are external data systems from which Koverse can ingest data.
  - name: Import Flows
    description: |
          Configure a Source to import external data to a specific Data Set.
  - name: Normalizations
    description: |
          Normalizations allow data to be modified as it is imported.
  - name: Import Jobs
    description: |
          Import jobs are used for importing data from an external Source into a Data Set.

          These jobs are executed by the Hadoop MapReduce or Spark frameworks asynchronously.
          Note that Koverse submits jobs to YARN which then executes them according to the way the YARN cluster is configured, which is typically a single queue so jobs are started on a first come, first serve basis.
          Jobs may run simultaneously as cluster resources allow.

          Jobs can be started or shutdown via these methods.
  - name: Import Schedules
    description: |
          Manage schedules for Import Flows.

          Schedules control how Import Jobs are started automatically.
          Import Jobs can still be created for Import Flows without schedules on an on-demand basis.
  - name: Transform Types
    description: |
          Get a list of transforms available in the system.
          Transform types specify which parameters are available for each type of Transform.
  - name: Transforms
    description: |
          Setup a partiular Transform Type to transform one or more data sets into a new Data Set.
  - name: Transform Jobs
    description: |
          Transform jobs process new data or all data in input Data Sets and write out results to an output Data Set.

          These jobs are executed by the Hadoop MapReduce or Spark frameworks asynchronously.
          Note that Koverse submits jobs to YARN which then executes them according to the way the YARN cluster is configured, which is typically a single queue so jobs are started on a first come, first serve basis.
          Jobs may run simultaneously as cluster resources allow.

          Jobs can be started or shutdown via these methods.
  - name: Transform Schedules
    description: |
          Manage schedules for Transforms.

          Schedules control how Transform Jobs are started automatically.
          Transforms Jobs can still be created for Transforms without schedules on an on-demand basis, or automatically as new data is ingested via Import Jobs into input Data Sets.
  - name: Sink Types
    description: |
          List and get available Sink types.
          Sinks are external systems to which Koverse can export data.
  - name: Sinks
    description: |
          Create, update, and delete Sinks.
          Sinks are external systems to which Koverse can export data.
  - name: Export Jobs
    description: |
          Export jobs write new data or all data in input Data Sets to an external Sink.

          These jobs are executed by the Hadoop MapReduce or Spark frameworks asynchronously.
          Note that Koverse submits jobs to YARN which then executes them according to the way the YARN cluster is configured, which is typically a single queue so jobs are started on a first come, first serve basis.
          Jobs may run simultaneously as cluster resources allow.

          Jobs can be started or shutdown via these methods.
  - name: Sink Schedules
    description: |
          Manage schedules for exporting to Sinks

          Schedules control how Export Jobs are started automatically.
          Export Jobs can still be created for Sinks without schedules on an on-demand basis or automatically as new data appears in input Data Sets.
  - name: Jobs
    description: |
          Start, monitor, and cancel data processing jobs.

          Data processing jobs are used for importing, transforming, and exporting data.
          These jobs are executed by the Hadoop MapReduce or Spark frameworks asynchronously.
          Koverse monitors the status of jobs, gathering information on the success or failure of a job, including gathering any error information about failed jobs.

          Jobs can be started or shutdown via these methods.
          Note that Koverse submits jobs to YARN which then executes them according to the way the YARN cluster is configured, which is typically a single queue so jobs are started on a first come, first serve basis.
          Jobs may run simultaneously as cluster resources allow.

          If two or more jobs will write on the same data set, one job may become blocked to allow the other job to complete its processing without inteference.
          In some cases a blocked job can be unblocked so it can proceed.

definitions:
  apiClient:
    $ref: schemas/apiClient.yaml
  dataSet:
    $ref: schemas/dataSet.yaml
  dataSetAttributes:
    $ref: schemas/dataSetAttributes.yaml
  dataSetAttributeUpdateList:
    $ref: schemas/dataSetAttributeUpdateList.yaml
  dataSetIndexingPolicy:
    $ref: schemas/dataSetIndexingPolicy.yaml
  dataSetJob:
    $ref: schemas/dataSetJob.yaml
  dataSetMetadata:
    $ref: schemas/dataSetMetadata.yaml
  dataSetPermission:
    $ref: schemas/dataSetPermission.yaml
  dataSetResult:
    $ref: schemas/dataSetResult.yaml
  dataSetSchemas:
    $ref: schemas/dataSetSchemas.yaml
  exportSchedule:
    $ref: schemas/exportSchedule.yaml
  fieldTypePair:
    $ref: schemas/fieldTypePair.yaml
  importFlow:
    $ref: schemas/importFlow.yaml
  importSchedule:
    $ref: schemas/importSchedule.yaml
  job:
    $ref: schemas/job.yaml
  login:
    $ref: schemas/login.yaml
  logout:
    $ref: schemas/logout.yaml
  normalization:
    $ref: schemas/normalization.yaml
  normalizationType:
    $ref: schemas/normalizationType.yaml
  parameter:
    $ref: schemas/parameter.yaml
  record:
    $ref: schemas/record.yaml
  sink:
    $ref: schemas/sink.yaml
  sinkType:
    $ref: schemas/sinkType.yaml
  sinkTypeDescription:
    $ref: schemas/sinkTypeDescription.yaml
  sinkTransformConfiguration:
    $ref: schemas/sinkTransformConfiguration.yaml
  source:
    $ref: schemas/source.yaml
  sourceInstance:
    $ref: schemas/sourceInstance.yaml
  sourceTypeDescription:
    $ref: schemas/sourceTypeDescription.yaml
  success:
    $ref: schemas/success.yaml
  transform:
    $ref: schemas/transform.yaml
  transformSchedule:
    $ref: schemas/transformSchedule.yaml
  transformType:
    $ref: schemas/transformType.yaml
  searchAutocomplete:
   $ref: schemas/searchAutocomplete.yaml
  user:
    $ref: schemas/user.yaml
  visualizationType:
    $ref: schemas/visualizationType.yaml

parameters:
  applicationId:
    name: applicationId
    in: path
    description: ID of the application
    required: true
    type: string
  dataSet:
    name: dataSet
    in: body
    description: Dataset attributes
    required: true
    schema:
      $ref: schemas/dataSet.yaml
  dataSetAttributeUpdateList:
    name: dataSetAttributeUpdateList
    in: body
    description: Attribute update list for masking attributes
    required: true
    schema:
      $ref: schemas/dataSetAttributeUpdateList.yaml
  dataSetId:
    name: dataSetId
    in: path
    description: ID of the data set
    required: true
    type: string
  dataSetMetadata:
    name: dataSetMetadata
    in: body
    description: Metadata of a dataset
    required: true
    schema:
      $ref: schemas/dataSetMetadata.yaml
  exportJobId:
    name: exportJobId
    in: path
    description: ID of the export job
    required: true
    type: string
  exportScheduleId:
    name: exportScheduleId
    in: path
    description: ID of an export schedule
    required: true
    type: string
  fileType:
    name: fileType
    in: path
    description: fileType to download
    required: true
    type: string
    enum:
      - json
      - csv
  importFlowId:
    name: importFlowId
    in: query
    description: ID of the import flow
    required: true
    type: string
  importFlowIdPath:
    name: importFlowId
    in: path
    description: ID of the import flow
    required: true
    type: string
  importJobId:
    name: importJobId
    in: path
    description: ID of the import job
    required: true
    type: integer
    format: int64
  importScheduleId:
    name: importScheduleId
    in: path
    description: ID of the import schedule
    required: true
    type: string
  jobId:
    name: jobId
    in: path
    description: ID of the job
    required: true
    type: string
  normalizationId:
    name: normalizationId
    in: path
    description: ID of the normalization
    required: true
    type: string
  objectQuery:
    name: query
    in: body
    required: true
    schema:
      $ref: schemas/objectQuery.yaml
  objectQueryNames:
    name: query
    in: body
    required: true
    schema:
      $ref: schemas/objectQueryNames.yaml
  permissionId:
    name: permissionId
    in: path
    description: ID of the permission
    required: true
    type: string
  recordId:
    name: recordId
    in: query
    description: ID of the record
    required: true
    type: string
  recordStyle:
    name: recordStyle
    in: query
    description: >
          Requests a more efficient representation of records
    required: false
    type: string
    enum: [2.2]
  sink:
    name: sink
    in: body
    required: true
    schema:
      $ref: "schemas/sink.yaml"
  sinkId:
    name: sinkId
    in: path
    description: ID of the sink
    type: integer
    required: true
  sinkTypeId:
    name: sinkTypeId
    in: path
    description: ID of the sink type
    required: true
    type: integer
    format: int64
  sourceId:
    name: sourceId
    in: path
    description: ID of the source
    required: true
    type: string
  sourceInstanceId:
    name: sourceInstanceId
    in: path
    description: ID of the source
    required: true
    type: string
  transformId:
    name: transformId
    in: path
    description: ID of a transform
    type: integer
    required: true
    format: int64
  transformJobId:
    name: transformJobId
    in: path
    required: true
    description: ID of the transform job
    type: integer
    format: int64
  transformScheduleId:
    name: transformScheduleId
    in: path
    required: true
    description: ID of the transform schedule
    type: integer
    format: int64

paths:
  /api/login:
    $ref: paths/login.yaml
  /api/logout:
    $ref: paths/logout.yaml
  /api/applications:
    $ref: paths/applications.yaml
  /api/applications/{applicationId}:
    $ref: paths/applicationId.yaml
  /api/dataSets:
    $ref: paths/dataSets.yaml
  /api/dataSets/{dataSetId}:
    $ref: paths/dataSetId.yaml
  /api/dataSets/{dataSetId}/attributeNames:
    $ref: paths/dataSetAttributeNames.yaml
  /api/dataSets/{dataSetId}/attributes:
    $ref: paths/dataSetAttributes.yaml
  /api/dataSets/{dataSetId}/clearDataSet:
    $ref: paths/dataSetClearDataSet.yaml
  #/api/dataSets/{dataSetId}/flatSchema:
  #  $ref: paths/dataSetFlatSchema.yaml
  /api/dataSets/{dataSetId}/indexingPolicy:
    $ref: paths/dataSetIndexingPolicies.yaml
  /api/dataSets/{dataSetId}/permissions:
    $ref: paths/dataSetPermissions.yaml
  /api/dataSets/{dataSetId}/permissions/{permissionId}:
    $ref: paths/datasetPermissionId.yaml
  /api/dataSets/{dataSetId}/jobs:
    $ref: paths/dataSetJobs.yaml
  /api/dataSets/{dataSetId}/records:
    $ref: paths/dataSetRecords.yaml
  #/api/dataSets/{dataSetId}/schema:
  #  $ref: paths/dataSetSchema.yaml
  /api/dataSets/{dataSetId}/repair:
    $ref: paths/dataSetRepair.yaml
  /api/dataSets/{dataSetId}/attributes/masking:
    $ref: paths/dataSetAttributesMasking.yaml
  /api/importFlows:
    $ref: paths/importFlows.yaml
  /api/importFlows/{importFlowId}:
    $ref: paths/importFlowId.yaml
  /api/importFlows/{importFlowId}/execute:
    $ref: paths/importFlowExecute.yaml
  /api/importFlows/{importFlowId}/normalizations:
    $ref: paths/importFlowNormalizations.yaml
  /api/importFlows/{importFlowId}/normalizations/{normalizationId}:
    $ref: paths/importFlowNormalizationsId.yaml
  /api/importFlows/{importFlowId}/schedules:
    $ref: paths/importFlowSchedules.yaml
  /api/importFlows/source/{sourceId}:
    $ref: paths/importFlowSourceId.yaml
  /api/importJobs/{importJobId}/shutdown:
    $ref: paths/importJobsShutdown.yaml
  /api/importSchedules:
    $ref: paths/importSchedules.yaml
  /api/importSchedules/{importScheduleId}:
    $ref: paths/importScheduleId.yaml
  /api/jobs:
    $ref: paths/jobs.yaml
  /api/jobs/{jobId}/shutdown:
    $ref: paths/jobsIdShutdown.yaml
  /api/jobs/{jobId}/unblock:
    $ref: paths/jobsIdUnblock.yaml
  /api/normalizationTypes:
    $ref: paths/normalizationTypes.yaml
  /api/permissions/dataset:
    $ref: paths/permissionsDataSet.yaml
  /api/permissions/system:
    $ref: paths/permissionsSystem.yaml
  /api/records:
    $ref: paths/records.yaml
  /api/sinks:
    $ref: paths/sinks.yaml
  /api/sinks/{sinkId}:
    $ref: paths/sinkId.yaml
  /api/sink/{sinkId}/runExport:
    $ref: paths/sinkRunExport.yaml
  /api/sinkSchedules:
    $ref: paths/sinkSchedules.yaml
  /api/sinkSchedules/{exportScheduleId}:
    $ref: paths/sinkScheduleId.yaml
  /api/sinkTypes:
    $ref: paths/sinkTypes.yaml
  /api/sinkTypes/{sinkTypeId}:
    $ref: paths/sinkTypeId.yaml
  /api/exportJobs/{exportJobId}/shutdown:
    $ref: paths/exportJobsShutdown.yaml
  /api/sourceInstances:
    $ref: paths/sourceInstances.yaml
  /api/sourceInstances/{sourceInstanceId}:
    $ref: paths/sourceInstanceId.yaml
  /api/sourceTypeDescriptions:
    $ref: paths/sourceTypeDescriptions.yaml
  /api/transformJobs/{transformJobId}/shutdown:
    $ref: paths/transformJobsShutdown.yaml
  /api/transforms:
    $ref: paths/transforms.yaml
  /api/transforms/{transformId}:
    $ref: paths/transformId.yaml
  /api/transforms/{transformId}/runTransform:
    $ref: paths/transformIdRunTransform.yaml
  /api/transforms/{transformId}/transformSchedules:
    $ref: paths/transformIdSchedules.yaml
  /api/transforms/{transformId}/transformSchedules/{transformScheduleId}:
    $ref: paths/transformIdScheduleId.yaml
  /api/transformTypes:
    $ref: paths/transformTypes.yaml
  /api/transformTypes/{typeId}:
    $ref: paths/transformTypeId.yaml
  /api/query/:
    $ref: paths/queryLucene.yaml
  /api/query/object:
    $ref: paths/queryObject.yaml
  /api/query/object/names:
    $ref: paths/queryObjectNames.yaml


  # this may not belong in a web app API
  #
  #  /api/dataSets/{dataSetId}/download/{fileType}:
  #    $ref: paths/dataSetDownload.yaml
  #  /api/dataSets/{dataSetId}/records:
  #    $ref: paths/records.yaml
  #/api/search/autocomplete:
  #  $ref: paths/searchAutocomplete.yaml
