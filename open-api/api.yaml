swagger: "2.0"
info:
  version: "3.2"
  title: Koverse REST API
  description: >
        Specification for interacting with the Koverse Rest API
host: demo.koverse.com

consumes:
  - application/json
produces:
  - application/json

tags:
  - name: Authentication
    description: |
          Methods for logging in individual users.

          These methods only apply when using Koverse's built-in user management.
  - name: Data Sets
    description: |
          Create, configure, and delete Data Sets.
  #- name: Data Set Schemas
  #  description: |
  #        Data Set Schemas
  - name: Data Set Attributes
    description: |
          List, inspect, and configure masking for data set attributes
  - name: Data Set Permissions
    description: |
          Control access to data in Data Sets.

          Access is controlled on a Group basis and includes actions such as writing to a Data Set, querying, deleting, changing configuration, etc.

          Note that these permissions are distinct from system permission which control things like which Groups can do things like create Data Sets, setup Transforms etc.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: System Permissions
    description: |
          Control access to system functionality.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Indexing Policies
    description: |
          Control how Koverse indexes data in a data set.

          This includes specifying which attributes to index, how data should be tokenized, and which combinations of attributes should be combined into Composite Indexes.
          The set of available indexes affects which kinds of interactive queries Koverse can support.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Query
    description: |
          Interactively query Data Sets.

          Queries are designed to retrieve a specific subset of records, in many cases in less than a second.
          Applications can call query methods in response to user requests.
  - name: Records
    description: |
          Write and retrieve individual records and sets of sample records.
  - name: Source Types
    description: |
          List available Source types.
          Source types specify which parameters are available for each type of Source.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Sources
    description: |
          Sources are external data systems from which Koverse can ingest data.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Import Flows
    description: |
          Configure a Source to import external data to a specific Data Set.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Normalizations
    description: |
          Normalizations allow data to be modified as it is imported.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Import Jobs
    description: |
          Import jobs are used for importing data from an external Source into a Data Set.

          These jobs are executed by the Hadoop MapReduce or Spark frameworks asynchronously.
          Note that Koverse submits jobs to YARN which then executes them according to the way the YARN cluster is configured, which is typically a single queue so jobs are started on a first come, first serve basis.
          Jobs may run simultaneously as cluster resources allow.

          Jobs can be started or shutdown via these methods.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Import Schedules
    description: |
          Manage schedules for Import Flows.

          Schedules control how Import Jobs are started automatically.
          Import Jobs can still be created for Import Flows without schedules on an on-demand basis.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Transform Types
    description: |
          Get a list of transforms available in the system.
          Transform types specify which parameters are available for each type of Transform.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Transforms
    description: |
          Setup a partiular Transform Type to transform one or more data sets into a new Data Set.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Transform Jobs
    description: |
          Transform jobs process new data or all data in input Data Sets and write out results to an output Data Set.

          These jobs are executed by the Hadoop MapReduce or Spark frameworks asynchronously.
          Note that Koverse submits jobs to YARN which then executes them according to the way the YARN cluster is configured, which is typically a single queue so jobs are started on a first come, first serve basis.
          Jobs may run simultaneously as cluster resources allow.

          Jobs can be started or shutdown via these methods.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Transform Schedules
    description: |
          Manage schedules for Transforms.

          Schedules control how Transform Jobs are started automatically.
          Transforms Jobs can still be created for Transforms without schedules on an on-demand basis, or automatically as new data is ingested via Import Jobs into input Data Sets.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Sink Types
    description: |
          List and get available Sink types.
          Sinks are external systems to which Koverse can export data.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Sinks
    description: |
          Create, update, and delete Sinks.
          Sinks are external systems to which Koverse can export data.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Export Jobs
    description: |
          Export jobs write new data or all data in input Data Sets to an external Sink.

          These jobs are executed by the Hadoop MapReduce or Spark frameworks asynchronously.
          Note that Koverse submits jobs to YARN which then executes them according to the way the YARN cluster is configured, which is typically a single queue so jobs are started on a first come, first serve basis.
          Jobs may run simultaneously as cluster resources allow.

          Jobs can be started or shutdown via these methods.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Sink Schedules
    description: |
          Manage schedules for exporting to Sinks

          Schedules control how Export Jobs are started automatically.
          Export Jobs can still be created for Sinks without schedules on an on-demand basis or automatically as new data appears in input Data Sets.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"
  - name: Jobs
    description: |
          Start, monitor, and cancel data processing jobs.

          Data processing jobs are used for importing, transforming, and exporting data.
          These jobs are executed by the Hadoop MapReduce or Spark frameworks asynchronously.
          Koverse monitors the status of jobs, gathering information on the success or failure of a job, including gathering any error information about failed jobs.

          Jobs can be started or shutdown via these methods.
          Note that Koverse submits jobs to YARN which then executes them according to the way the YARN cluster is configured, which is typically a single queue so jobs are started on a first come, first serve basis.
          Jobs may run simultaneously as cluster resources allow.

          If two or more jobs will write on the same data set, one job may become blocked to allow the other job to complete its processing without inteference.
          In some cases a blocked job can be unblocked so it can proceed.
    externalDocs:
      description: "Deprecated. Please see Automation Guide for API usage."
      url: "dev/automation/automationguide.html"

definitions:
  500servererror:
    $ref: schemas/servererror.yaml
  400servererror:
    $ref: schemas/badrequest.yaml
  apiClient:
    $ref: schemas/apiClient.yaml
  dataSet:
    $ref: schemas/dataSet.yaml
  dataSetAttributes:
    $ref: schemas/dataSetAttributes.yaml
  dataSetAttributeUpdateList:
    $ref: schemas/dataSetAttributeUpdateList.yaml
  dataSetIndexingPolicy:
    $ref: schemas/dataSetIndexingPolicy.yaml
  dataSetJob:
    $ref: schemas/dataSetJob.yaml
  dataSetMetadata:
    $ref: schemas/dataSetMetadata.yaml
  dataSetPermission:
    $ref: schemas/dataSetPermission.yaml
  dataSetResult:
    $ref: schemas/dataSetResult.yaml
  dataSetSchemas:
    $ref: schemas/dataSetSchemas.yaml
  exportSchedule:
    $ref: schemas/exportSchedule.yaml
  fieldTypePair:
    $ref: schemas/fieldTypePair.yaml
  importFlow:
    $ref: schemas/importFlow.yaml
  importSchedule:
    $ref: schemas/importSchedule.yaml
  job:
    $ref: schemas/job.yaml
  login:
    $ref: schemas/login.yaml
  logout:
    $ref: schemas/logout.yaml
  normalization:
    $ref: schemas/normalization.yaml
  normalizationType:
    $ref: schemas/normalizationType.yaml
  parameter:
    $ref: schemas/parameter.yaml
  record:
    $ref: schemas/oldjsonrecordformat.yaml
  sink:
    $ref: schemas/sink.yaml
  sinkType:
    $ref: schemas/sinkType.yaml
  sinkTypeDescription:
    $ref: schemas/sinkTypeDescription.yaml
  sinkTransformConfiguration:
    $ref: schemas/sinkTransformConfiguration.yaml
  source:
    $ref: schemas/source.yaml
  sourceInstance:
    $ref: schemas/sourceInstance.yaml
  sourceTypeDescription:
    $ref: schemas/sourceTypeDescription.yaml
  success:
    $ref: schemas/success.yaml
  transform:
    $ref: schemas/transform.yaml
  transformSchedule:
    $ref: schemas/transformSchedule.yaml
  transformType:
    $ref: schemas/transformType.yaml
  searchAutocomplete:
   $ref: schemas/searchAutocomplete.yaml
  user:
    $ref: schemas/user.yaml
  visualizationType:
    $ref: schemas/visualizationType.yaml

parameters:
  applicationId:
    name: applicationId
    in: path
    description: ID of the application
    required: true
    type: string
  dataSet:
    name: dataSet
    in: body
    description: Dataset attributes
    required: true
    schema:
      $ref: schemas/dataSet.yaml
  dataSetAttributeUpdateList:
    name: dataSetAttributeUpdateList
    in: body
    description: Attribute update list for masking attributes
    required: true
    schema:
      $ref: schemas/dataSetAttributeUpdateList.yaml
  dataSetId:
    name: dataSetId
    in: path
    description: ID of the data set
    required: true
    type: string
  dataSetMetadata:
    name: dataSetMetadata
    in: body
    description: Metadata of a dataset
    required: true
    schema:
      $ref: schemas/dataSetMetadata.yaml
  exportJobId:
    name: exportJobId
    in: path
    description: ID of the export job
    required: true
    type: string
  exportScheduleId:
    name: exportScheduleId
    in: path
    description: ID of an export schedule
    required: true
    type: string
  fileType:
    name: fileType
    in: path
    description: fileType to download
    required: true
    type: string
    enum:
      - json
      - csv
  importFlowId:
    name: importFlowId
    in: query
    description: ID of the import flow
    required: true
    type: string
  importFlowIdPath:
    name: importFlowId
    in: path
    description: ID of the import flow
    required: true
    type: string
  importJobId:
    name: importJobId
    in: path
    description: ID of the import job
    required: true
    type: integer
    format: int64
  importScheduleId:
    name: importScheduleId
    in: path
    description: ID of the import schedule
    required: true
    type: string
  jobId:
    name: jobId
    in: path
    description: ID of the job
    required: true
    type: string
#  maxRecords:
#    name: maxRecords
#    in: query
#    description: max number of records to return
  maxStringValueLength:
    name: maxStringValueLength
    in: query
    description: length to truncate string values, default is -1 for 'no truncate'
    required: false
    type: integer
    format: int64
  normalizationId:
    name: normalizationId
    in: path
    description: ID of the normalization
    required: true
    type: string
  objectQuery:
    name: query
    in: body
    required: true
    schema:
      $ref: schemas/objectQuery.yaml
  objectQueryNames:
    name: query
    in: body
    required: true
    schema:
      $ref: schemas/objectQueryNames.yaml
  permissionId:
    name: permissionId
    in: path
    description: ID of the permission
    required: true
    type: string
  recordId:
    name: recordId
    in: query
    description: ID of the record
    required: true
    type: string
  recordStyle:
    name: recordStyle
    in: query
    description: |
          Requests a more efficient representation of records
    required: false
    type: string
    enum: [2.2]
  removeByteArrayFieldValues:
    name: removeByteArrayFieldValues
    in: query
    description: |
          Removes values with datatype byte array
    required: false
    type: boolean
  sink:
    name: sink
    in: body
    required: true
    schema:
      $ref: "schemas/sink.yaml"
  sinkId:
    name: sinkId
    in: path
    description: ID of the sink
    type: integer
    required: true
  sinkTypeId:
    name: sinkTypeId
    in: path
    description: ID of the sink type
    required: true
    type: integer
    format: int64
  sourceId:
    name: sourceId
    in: path
    description: ID of the source
    required: true
    type: string
  sourceInstanceId:
    name: sourceInstanceId
    in: path
    description: ID of the source
    required: true
    type: string
  transformId:
    name: transformId
    in: path
    description: ID of a transform
    type: integer
    required: true
    format: int64
  transformJobId:
    name: transformJobId
    in: path
    required: true
    description: ID of the transform job
    type: integer
    format: int64
  transformScheduleId:
    name: transformScheduleId
    in: path
    required: true
    description: ID of the transform schedule
    type: integer
    format: int64

paths:
  /api/v1/login:
    $ref: paths/login.yaml
  /api/v1/logout:
    $ref: paths/logout.yaml
  /api/v1/applications:
    $ref: paths/applications.yaml
  /api/v1/applications/{applicationId}:
    $ref: paths/applicationId.yaml
  /api/v1/data-sets:
    $ref: paths/dataSets.yaml
  /api/v1/data-sets/{dataSetId}:
    $ref: paths/dataSetId.yaml
  /api/v1/data-sets/{dataSetId}/attribute-names:
    $ref: paths/dataSetAttributeNames.yaml
  /api/v1/data-sets/{dataSetId}/attributes:
    $ref: paths/dataSetAttributes.yaml
  /api/v1/data-sets/{dataSetId}/clear:
    $ref: paths/dataSetClearDataSet.yaml
  #/api/v1/data-sets/{dataSetId}/flatSchema:
  #  $ref: paths/dataSetFlatSchema.yaml
  /api/v1/data-sets/{dataSetId}/indexing-policy:
    $ref: paths/dataSetIndexingPolicies.yaml
  /api/v1/data-sets/{dataSetId}/jobs:
    $ref: paths/dataSetJobs.yaml
  /api/v1/data-sets/{dataSetId}/records:
    $ref: paths/dataSetRecords.yaml
  /api/v2/data-sets/{dataSetId}/records:
    $ref: paths/dataSetRecordsV2.yaml
  #/api/v1/data-sets/{dataSetId}/schema:
  #  $ref: paths/dataSetSchema.yaml
  /api/v1/data-sets/{dataSetId}/repair:
    $ref: paths/dataSetRepair.yaml
  /api/v1/data-sets/{dataSetId}/attributes/masking:
    $ref: paths/dataSetAttributesMasking.yaml
  /api/v1/import-flows:
    $ref: paths/importFlows.yaml
  /api/v1/import-flows/{importFlowId}:
    $ref: paths/importFlowId.yaml
  /api/v1/import-flows/{importFlowId}/execute:
    $ref: paths/importFlowExecute.yaml
  /api/v1/import-flows/{importFlowId}/normalizations:
    $ref: paths/importFlowNormalizations.yaml
  /api/v1/import-flows/{importFlowId}/normalizations/{normalizationId}:
    $ref: paths/importFlowNormalizationsId.yaml
  /api/v1/import-flows/{importFlowId}/schedules:
    $ref: paths/importFlowSchedules.yaml
  /api/v1/import-flows/source/{sourceId}:
    $ref: paths/importFlowSourceId.yaml
  /api/v1/import-flows/{importJobId}/shutdown:
    $ref: paths/importJobsShutdown.yaml
  /api/v1/import-schedules:
    $ref: paths/importSchedules.yaml
  /api/v1/import-schedules/{importScheduleId}:
    $ref: paths/importScheduleId.yaml
  /api/v1/jobs:
    $ref: paths/jobs.yaml
  /api/v1/jobs/{jobId}/shutdown:
    $ref: paths/jobsIdShutdown.yaml
  /api/v1/jobs/{jobId}/unblock:
    $ref: paths/jobsIdUnblock.yaml
  /api/v1/permissions:
    $ref: paths/permissions.yaml
  /api/v1/permissions/{permissionId}:
    $ref: paths/datasetPermissionId.yaml
  /api/v1/normalization-types:
    $ref: paths/normalizationTypes.yaml
  /api/v1/permissions/dataset:
    $ref: paths/permissionsDataSet.yaml
  /api/v1/permissions/system:
    $ref: paths/permissionsSystem.yaml
  /api/v1/records:
    $ref: paths/records.yaml
  /api/v1/sinks:
    $ref: paths/sinks.yaml
  /api/v1/sinks/{sinkId}:
    $ref: paths/sinkId.yaml
  /api/v1/sinks/{sinkId}/run-export:
    $ref: paths/sinkRunExport.yaml
  /api/v1/sink-schedules:
    $ref: paths/sinkSchedules.yaml
  /api/v1/sink-schedules/{exportScheduleId}:
    $ref: paths/sinkScheduleId.yaml
  /api/v1/sink-types:
    $ref: paths/sinkTypes.yaml
  /api/v1/sink-types/{sinkTypeId}:
    $ref: paths/sinkTypeId.yaml
  /api/v1/export-jobs/{exportJobId}/shutdown:
    $ref: paths/exportJobsShutdown.yaml
  /api/v1/source-instances:
    $ref: paths/sourceInstances.yaml
  /api/v1/source-instances/{sourceInstanceId}:
    $ref: paths/sourceInstanceId.yaml
  /api/v1/source-type-descriptions:
    $ref: paths/sourceTypeDescriptions.yaml
  /api/v1/transform-jobs/{transformJobId}/shutdown:
    $ref: paths/transformJobsShutdown.yaml
  /api/v1/transforms:
    $ref: paths/transforms.yaml
  /api/v1/transforms/{transformId}:
    $ref: paths/transformId.yaml
  /api/v1/transforms/{transformId}/run-transform:
    $ref: paths/transformIdRunTransform.yaml
  /api/v1/transforms/{transformId}/schedules:
    $ref: paths/transformIdSchedules.yaml
  /api/v1/transforms/{transformId}/schedules/{transformScheduleId}:
    $ref: paths/transformIdScheduleId.yaml
  /api/v1/transform-types:
    $ref: paths/transformTypes.yaml
  /api/v1/transform-types/{typeId}:
    $ref: paths/transformTypeId.yaml
  /api/v1/queries/lucene:
    $ref: paths/queryLucene.yaml
  /api/v1/queries/object:
    $ref: paths/queryObject.yaml
  /api/v1/queries/object/names:
    $ref: paths/queryObjectNames.yaml
  /api/v2/queries/lucene:
    $ref: paths/queryLuceneV2.yaml
  /api/v2/queries/object:
    $ref: paths/queryObjectV2.yaml
  /api/v2/queries/object/names:
    $ref: paths/queryObjectNamesV2.yaml


  # this may not belong in a web app API
  #
  #  /api/v1/data-sets/{dataSetId}/download/{fileType}:
  #    $ref: paths/dataSetDownload.yaml
  #  /api/v1/data-sets/{dataSetId}/records:
  #    $ref: paths/records.yaml
  #/api/v1/search/autocomplete:
  #  $ref: paths/searchAutocomplete.yaml
